#!/usr/bin/env python3
"""
Import Update Script

Systematically updates import paths during Phase 2 service consolidation.
Uses dependency mapping data to ensure safe, comprehensive updates.
"""

import ast
import json
import re
import shutil
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
from dataclasses import dataclass
import argparse


@dataclass
class ImportMapping:
    """Represents an import path mapping from old to new location"""
    old_path: str
    new_path: str
    description: str
    risk_level: str  # low, medium, high


@dataclass
class UpdateResult:
    """Result of an import update operation"""
    file_path: str
    updates_made: int
    errors: List[str]
    warnings: List[str]
    backup_created: bool


class ImportUpdateManager:
    """Manages systematic import path updates during consolidation"""

    def __init__(self, project_root: Path, dry_run: bool = True):
        self.project_root = project_root
        self.agentpm_root = project_root / "aipm-cli" / "aipm_cli"
        self.dry_run = dry_run
        self.backup_dir = project_root / "backups" / "phase2_import_updates"

        # Load dependency data for safer updates
        self.dependency_data = self._load_dependency_data()

        # Initialize consolidation mappings based on Phase 2 plan
        self.consolidation_mappings = self._initialize_consolidation_mappings()

    def _load_dependency_data(self) -> Optional[Dict]:
        """Load dependency mapping data"""
        dependency_file = self.project_root / "docs" / "todo" / "service-dependencies.json"
        if dependency_file.exists():
            with open(dependency_file, 'r') as f:
                return json.load(f)
        return None

    def _initialize_consolidation_mappings(self) -> List[ImportMapping]:
        """Initialize Phase 2 consolidation mappings"""
        return [
            # Core Services Consolidation
            ImportMapping(
                old_path="aipm_cli.services.context.service",
                new_path="aipm_cli.core.context_service",
                description="Consolidate context services into core module",
                risk_level="medium"
            ),
            ImportMapping(
                old_path="aipm_cli.services.database_router",
                new_path="aipm_cli.core.database_service",
                description="Consolidate database routing into core service",
                risk_level="high"  # 5 dependents
            ),

            # Intelligence Services Consolidation
            ImportMapping(
                old_path="aipm_cli.services.context_scorecard",
                new_path="aipm_cli.intelligence.context_intelligence",
                description="Consolidate context intelligence services",
                risk_level="medium"
            ),
            ImportMapping(
                old_path="aipm_cli.services.context_confidence",
                new_path="aipm_cli.intelligence.context_intelligence",
                description="Merge context confidence into intelligence module",
                risk_level="low"
            ),
            ImportMapping(
                old_path="aipm_cli.services.pm_methodology_context",
                new_path="aipm_cli.intelligence.methodology_intelligence",
                description="Consolidate PM methodology intelligence",
                risk_level="low"
            ),

            # Workflow Services Consolidation
            ImportMapping(
                old_path="aipm_cli.services.action_gates",
                new_path="aipm_cli.workflow.gate_service",
                description="Consolidate action gates into workflow module",
                risk_level="low"
            ),
            ImportMapping(
                old_path="aipm_cli.services.readiness_assessment",
                new_path="aipm_cli.workflow.assessment_service",
                description="Consolidate readiness assessment",
                risk_level="low"
            ),
            ImportMapping(
                old_path="aipm_cli.services.kanban_workflow",
                new_path="aipm_cli.workflow.kanban_service",
                description="Consolidate Kanban workflow management",
                risk_level="low"
            ),

            # Utility Services Consolidation
            ImportMapping(
                old_path="aipm_cli.services.file_operation_classifier",
                new_path="aipm_cli.utils.file_service",
                description="Consolidate file operations into utilities",
                risk_level="low"
            ),
            ImportMapping(
                old_path="aipm_cli.services.init_questionnaire",
                new_path="aipm_cli.utils.questionnaire_service",
                description="Consolidate questionnaire utilities",
                risk_level="low"
            ),
            ImportMapping(
                old_path="aipm_cli.services.gate_recovery_system",
                new_path="aipm_cli.utils.recovery_service",
                description="Consolidate recovery system utilities",
                risk_level="low"
            ),

            # Fix remaining old model references
            ImportMapping(
                old_path="aipm_cli.models",
                new_path="aipm_cli.services.database.models",
                description="Fix remaining old model imports",
                risk_level="high"  # Critical for system functionality
            ),
        ]

    def create_backup(self, file_path: Path) -> bool:
        """Create backup of file before modification"""
        if self.dry_run:
            return True

        try:
            self.backup_dir.mkdir(parents=True, exist_ok=True)

            # Create relative backup path maintaining directory structure
            rel_path = file_path.relative_to(self.project_root)
            backup_path = self.backup_dir / rel_path
            backup_path.parent.mkdir(parents=True, exist_ok=True)

            shutil.copy2(file_path, backup_path)
            return True
        except Exception as e:
            print(f"âš ï¸ Failed to create backup for {file_path}: {e}")
            return False

    def analyze_import_statement(self, line: str) -> Tuple[Optional[str], Optional[str], str]:
        """Analyze an import statement to extract module and format"""
        line = line.strip()

        # Pattern for "from module import ..."
        from_match = re.match(r'^from\s+([a-zA-Z_][a-zA-Z0-9_.]*)\s+import\s+(.+)', line)
        if from_match:
            return from_match.group(1), from_match.group(2), 'from_import'

        # Pattern for "import module"
        import_match = re.match(r'^import\s+([a-zA-Z_][a-zA-Z0-9_.]*)', line)
        if import_match:
            return import_match.group(1), None, 'direct_import'

        return None, None, 'unknown'

    def update_import_in_line(self, line: str, mapping: ImportMapping) -> Tuple[str, bool]:
        """Update a single import line according to mapping"""
        original_line = line
        modified = False

        # Extract import information
        module, imports, import_type = self.analyze_import_statement(line)

        if not module:
            return line, False

        # Check if this line matches our mapping
        if module == mapping.old_path or module.startswith(mapping.old_path + '.'):
            if import_type == 'from_import':
                new_line = f"from {mapping.new_path} import {imports}"
            elif import_type == 'direct_import':
                new_line = f"import {mapping.new_path}"
            else:
                return line, False

            return new_line, True

        return line, False

    def update_file_imports(self, file_path: Path, mappings: List[ImportMapping]) -> UpdateResult:
        """Update imports in a single file"""
        result = UpdateResult(
            file_path=str(file_path),
            updates_made=0,
            errors=[],
            warnings=[],
            backup_created=False
        )

        try:
            # Read file content
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            # Create backup before modifying
            if not self.dry_run:
                result.backup_created = self.create_backup(file_path)
                if not result.backup_created:
                    result.errors.append("Failed to create backup - skipping file")
                    return result

            # Process each line
            updated_lines = []
            for line_num, line in enumerate(lines, 1):
                updated_line = line
                line_modified = False

                # Try each mapping
                for mapping in mappings:
                    if not line_modified:  # Only apply first matching mapping
                        new_line, was_modified = self.update_import_in_line(updated_line, mapping)
                        if was_modified:
                            updated_line = new_line
                            line_modified = True
                            result.updates_made += 1

                            # Add warning for high-risk updates
                            if mapping.risk_level == 'high':
                                result.warnings.append(
                                    f"Line {line_num}: High-risk update - {mapping.description}"
                                )

                updated_lines.append(updated_line)

            # Write updated content if not dry run
            if not self.dry_run and result.updates_made > 0:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.writelines(updated_lines)

        except Exception as e:
            result.errors.append(f"Failed to process file: {str(e)}")

        return result

    def discover_files_to_update(self) -> List[Path]:
        """Discover Python files that might need import updates"""
        python_files = []

        # Search in main aipm_cli directory
        if self.agentpm_root.exists():
            for py_file in self.agentpm_root.rglob("*.py"):
                if py_file.name != "__init__.py":  # Handle __init__.py separately if needed
                    python_files.append(py_file)

        # Also check test files
        test_dirs = [
            self.project_root / "aipm-cli" / "tests-BAK",
            self.project_root / "aipm-cli" / "test"  # Alternative test directory
        ]

        for test_dir in test_dirs:
            if test_dir.exists():
                for py_file in test_dir.rglob("*.py"):
                    python_files.append(py_file)

        return python_files

    def run_consolidation_update(self, target_mappings: Optional[List[str]] = None) -> Dict[str, List[UpdateResult]]:
        """Run import updates for Phase 2 consolidation"""
        print("ğŸ”„ AIPM Import Update Script - Phase 2 Consolidation")
        print("=" * 60)

        if self.dry_run:
            print("ğŸ” DRY RUN MODE - No files will be modified")
        else:
            print("âš ï¸  LIVE MODE - Files will be modified (backups created)")

        print()

        # Filter mappings if specific targets requested
        mappings_to_apply = self.consolidation_mappings
        if target_mappings:
            mappings_to_apply = [m for m in self.consolidation_mappings
                               if any(target in m.old_path for target in target_mappings)]
            print(f"ğŸ¯ Targeting specific mappings: {target_mappings}")

        print(f"ğŸ“‹ Applying {len(mappings_to_apply)} import mappings")

        # Show mappings to be applied
        for mapping in mappings_to_apply:
            risk_emoji = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸ”´"}.get(mapping.risk_level, "â“")
            print(f"   {risk_emoji} {mapping.old_path} â†’ {mapping.new_path}")

        print()

        # Discover files to update
        files_to_update = self.discover_files_to_update()
        print(f"ğŸ“ Found {len(files_to_update)} Python files to check")

        # Group results by risk level
        results = {
            "high_risk": [],
            "medium_risk": [],
            "low_risk": [],
            "no_changes": []
        }

        # Process each file
        total_updates = 0
        total_files_modified = 0

        for file_path in files_to_update:
            try:
                result = self.update_file_imports(file_path, mappings_to_apply)

                if result.updates_made > 0:
                    total_updates += result.updates_made
                    total_files_modified += 1

                    # Categorize by risk level
                    has_high_risk = any("High-risk update" in w for w in result.warnings)
                    has_medium_risk = any(m.risk_level == "medium" for m in mappings_to_apply)

                    if has_high_risk:
                        results["high_risk"].append(result)
                    elif has_medium_risk:
                        results["medium_risk"].append(result)
                    else:
                        results["low_risk"].append(result)
                else:
                    results["no_changes"].append(result)

            except Exception as e:
                print(f"ğŸ’¥ Error processing {file_path}: {e}")

        # Summary
        print(f"\nğŸ“Š Update Summary:")
        print(f"   - Total files processed: {len(files_to_update)}")
        print(f"   - Files modified: {total_files_modified}")
        print(f"   - Total import updates: {total_updates}")
        print(f"   - High-risk updates: {len(results['high_risk'])}")
        print(f"   - Medium-risk updates: {len(results['medium_risk'])}")
        print(f"   - Low-risk updates: {len(results['low_risk'])}")

        return results

    def generate_update_report(self, results: Dict[str, List[UpdateResult]]) -> str:
        """Generate comprehensive update report"""
        report = f"""# Import Update Report - Phase 2 Consolidation
Generated: September 2024
Mode: {'DRY RUN' if self.dry_run else 'LIVE UPDATE'}

## ğŸ“Š Summary Statistics

"""

        total_files = sum(len(result_list) for result_list in results.values())
        files_modified = sum(len(result_list) for key, result_list in results.items() if key != "no_changes")
        total_updates = sum(sum(r.updates_made for r in result_list) for result_list in results.values())

        report += f"""**Total Files Processed**: {total_files}
**Files Modified**: {files_modified}
**Total Import Updates**: {total_updates}

## ğŸš¨ Risk Assessment Results

"""

        for risk_level in ["high_risk", "medium_risk", "low_risk"]:
            risk_results = results[risk_level]
            if risk_results:
                emoji = {"high_risk": "ğŸ”´", "medium_risk": "ğŸŸ¡", "low_risk": "ğŸŸ¢"}[risk_level]
                report += f"### {risk_level.replace('_', ' ').title()} Updates {emoji}\n\n"

                for result in risk_results:
                    report += f"**File**: `{result.file_path}`\n"
                    report += f"- Updates made: {result.updates_made}\n"

                    if result.warnings:
                        report += "- Warnings:\n"
                        for warning in result.warnings:
                            report += f"  - {warning}\n"

                    if result.errors:
                        report += "- Errors:\n"
                        for error in result.errors:
                            report += f"  - {error}\n"

                    report += f"- Backup created: {'âœ…' if result.backup_created else 'âŒ'}\n\n"

        report += """
## ğŸ”„ Next Steps

### If this was a dry run:
1. Review high-risk updates carefully
2. Run live update with `--live` flag
3. Test critical services after updates
4. Validate application functionality

### If this was a live update:
1. Run comprehensive test suite
2. Verify service imports work correctly
3. Check for any runtime import errors
4. Validate Phase 2 consolidation is working

### Rollback procedure:
1. Restore from backups in `backups/phase2_import_updates/`
2. Run git reset if needed
3. Revert to previous working state

## ğŸ“‹ Consolidation Mappings Applied

"""

        for mapping in self.consolidation_mappings:
            risk_emoji = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸ”´"}.get(mapping.risk_level, "â“")
            report += f"- {risk_emoji} **{mapping.old_path}** â†’ **{mapping.new_path}**\n"
            report += f"  - {mapping.description}\n"

        return report


def main():
    """Main function with CLI interface"""
    parser = argparse.ArgumentParser(description="AIPM Import Update Script for Phase 2 Consolidation")
    parser.add_argument("--live", action="store_true", help="Apply updates (default is dry-run)")
    parser.add_argument("--target", nargs="*", help="Target specific import paths")
    parser.add_argument("--report-only", action="store_true", help="Generate report from existing results")

    args = parser.parse_args()

    project_root = Path(__file__).parent.parent
    manager = ImportUpdateManager(project_root, dry_run=not args.live)

    if args.report_only:
        print("ğŸ“„ Generating report from existing results...")
        # This would load previous results if available
        return

    # Run the update process
    results = manager.run_consolidation_update(args.target)

    # Generate and save report
    report = manager.generate_update_report(results)

    report_path = project_root / "docs" / "todo" / f"import-update-report-{'live' if args.live else 'dry-run'}.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nğŸ“„ Report saved to: {report_path}")

    if not args.live:
        print("\nğŸ’¡ This was a dry run. Use --live to apply changes.")
        print("ğŸ’¡ Review the report before running live updates.")


if __name__ == "__main__":
    main()