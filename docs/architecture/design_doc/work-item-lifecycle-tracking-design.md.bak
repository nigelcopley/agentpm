# WorkItem Lifecycle Tracking System - Comprehensive Design

**Status**: Design Proposal
**Created**: 2025-10-17
**Context**: Enhanced WorkItem lifecycle tracking for phase transitions, quality metrics, and effort tracking

---

## Executive Summary

This document proposes three architectural options for comprehensive WorkItem lifecycle tracking, evaluating trade-offs between query performance, storage efficiency, migration complexity, and flexibility. The system needs to track phase history, blockers, quality metrics, effort tracking, and architectural decisions while maintaining backward compatibility.

---

## Current State Analysis

### Existing Schema

```sql
-- Current work_items table (simplified)
CREATE TABLE work_items (
    id INTEGER PRIMARY KEY,
    project_id INTEGER,
    parent_work_item_id INTEGER,
    name TEXT,
    description TEXT,
    type TEXT,  -- FEATURE, ANALYSIS, OBJECTIVE, RESEARCH
    business_context TEXT,
    metadata TEXT,  -- JSON blob (currently: {"continuous": {...}})
    effort_estimate_hours REAL,
    priority INTEGER,
    status TEXT,  -- DRAFT, READY, ACTIVE, REVIEW, DONE, ARCHIVED
    is_continuous BOOLEAN,
    phase TEXT,  -- D1_DISCOVERY, P1_PLAN, I1_IMPLEMENTATION, R1_REVIEW, O1_OPERATIONS, E1_EVOLUTION
    due_date TIMESTAMP,
    not_before TIMESTAMP,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

### Existing Event System

```sql
-- Events table (comprehensive event tracking)
CREATE TABLE events (
    id INTEGER PRIMARY KEY,
    event_type TEXT,  -- 40+ types: task.started, work_item.completed, etc.
    event_category TEXT,  -- workflow, tool_usage, decision, reasoning, error
    event_severity TEXT,  -- debug, info, warning, error, critical
    session_id INTEGER,
    timestamp TIMESTAMP,
    source TEXT,
    event_data TEXT,  -- JSON blob with typed schemas
    project_id INTEGER,
    work_item_id INTEGER,
    task_id INTEGER,
    created_at TIMESTAMP
);
```

### Current Tracking Capabilities

**✅ What We Track**:
- Phase transitions (via status changes)
- Status history (via events table: work_item.started, work_item.completed)
- Session activity (via events linked to sessions)
- Tool usage (via events: tool.read_file, tool.write_file)
- Decisions (via events: decision.made)

**❌ What We Don't Track**:
- Phase duration metrics (time in each phase)
- Blocker history (when blocked, why, how long)
- Quality metrics evolution (test coverage over time)
- Effort variance tracking (estimated vs actual)
- Explicit architecture decisions tied to work items
- Phase-specific artifacts and evidence

---

## Design Requirements

### Functional Requirements

**FR-1: Phase History Tracking**
- Track entry/exit timestamps for each phase
- Calculate duration in each phase
- Support phase re-entries (for rework cycles)

**FR-2: Blocker Tracking**
- Record blocker reasons with timestamps
- Track blocker resolution and duration
- Support multiple concurrent blockers

**FR-3: Quality Metrics**
- Track test coverage evolution
- Record code quality scores
- Track review iterations and defect counts

**FR-4: Effort Tracking**
- Record estimated vs actual effort
- Calculate variance percentage
- Support time-boxing validation

**FR-5: Decision Recording**
- Capture architectural decisions
- Store rationale and alternatives
- Link decisions to work items

### Non-Functional Requirements

**NFR-1: Query Performance**
- Filter work items by coverage > 0.8: < 100ms
- Get phase duration for work item: < 50ms
- Get blocker history for project: < 200ms

**NFR-2: Storage Efficiency**
- Minimize data duplication
- Use normalized storage for frequently-queried fields
- JSON for flexible/evolving metadata

**NFR-3: Migration Complexity**
- Backward compatible (no data loss)
- Incremental adoption (new fields optional)
- Rollback support

**NFR-4: Flexibility**
- Easy to add new metrics without schema changes
- Support evolving quality frameworks
- Extensible for future phases/gates

---

## Option A: Enhanced Metadata Structure (JSON-Centric)

### Architecture

**Philosophy**: Store lifecycle data in structured JSON metadata field, add indexed columns for frequently-queried values.

### Schema Changes

```sql
-- Add indexed columns for common queries
ALTER TABLE work_items ADD COLUMN phase_entered_at TIMESTAMP;
ALTER TABLE work_items ADD COLUMN status_entered_at TIMESTAMP;
ALTER TABLE work_items ADD COLUMN current_blocker_reason TEXT;
ALTER TABLE work_items ADD COLUMN blocked_since TIMESTAMP;
ALTER TABLE work_items ADD COLUMN test_coverage REAL;  -- 0.0 to 1.0
ALTER TABLE work_items ADD COLUMN actual_hours REAL;

-- Create indexes for performance
CREATE INDEX idx_work_items_test_coverage ON work_items(test_coverage) WHERE test_coverage IS NOT NULL;
CREATE INDEX idx_work_items_actual_hours ON work_items(actual_hours) WHERE actual_hours IS NOT NULL;
CREATE INDEX idx_work_items_blocked ON work_items(blocked_since) WHERE blocked_since IS NOT NULL;
```

### Enhanced Metadata JSON Structure

```json
{
  "lifecycle": {
    "phase_history": [
      {
        "phase": "D1_DISCOVERY",
        "entered_at": "2025-10-16T10:00:00Z",
        "exited_at": "2025-10-16T14:30:00Z",
        "duration_hours": 4.5,
        "artifacts": [
          {"type": "6w_analysis", "path": ".agentpm/artifacts/wi_60_6w.md"}
        ],
        "gates_passed": ["context_quality", "why_value_captured"]
      },
      {
        "phase": "P1_PLAN",
        "entered_at": "2025-10-17T09:00:00Z",
        "exited_at": null,  -- Currently in this phase
        "duration_hours": null,
        "artifacts": [],
        "gates_passed": []
      }
    ],
    "status_history": [
      {"status": "draft", "entered_at": "2025-10-16T10:00:00Z", "exited_at": "2025-10-17T09:00:00Z"},
      {"status": "ready", "entered_at": "2025-10-17T09:00:00Z", "exited_at": null}
    ],
    "blockers": [
      {
        "reason": "API approval needed from security team",
        "blocked_at": "2025-10-17T11:00:00Z",
        "resolved_at": "2025-10-17T13:00:00Z",
        "duration_hours": 2.0,
        "resolution": "Security team approved with rate limiting requirement"
      }
    ]
  },
  "quality_metrics": {
    "test_coverage_history": [
      {"timestamp": "2025-10-17T14:00:00Z", "coverage": 0.78},
      {"timestamp": "2025-10-17T16:00:00Z", "coverage": 0.92}
    ],
    "code_quality_score": 0.88,
    "review_iterations": 2,
    "defects_found": 3,
    "defects_resolved": 3,
    "static_analysis": {
      "complexity": "moderate",
      "violations": 0
    }
  },
  "effort_tracking": {
    "estimated_hours": 40.0,
    "actual_hours": 45.5,
    "variance_percent": 13.75,
    "time_entries": [
      {"date": "2025-10-16", "hours": 8.0, "phase": "D1_DISCOVERY"},
      {"date": "2025-10-17", "hours": 7.5, "phase": "P1_PLAN"}
    ]
  },
  "decisions": [
    {
      "decision": "Use PostgreSQL for relational data",
      "rationale": "Better JSON support than MySQL, ACID guarantees, mature ecosystem",
      "alternatives_considered": ["MySQL", "MongoDB", "SQLite"],
      "timestamp": "2025-10-16T12:00:00Z",
      "decision_maker": "aipm-database-developer"
    }
  ],
  "continuous": {
    "category": "bug_backlog",  -- Existing continuous metadata
    "created_at": "2025-10-01T00:00:00Z"
  }
}
```

### Implementation

```python
# agentpm/core/database/methods/work_items.py

def record_phase_transition(
    service: DatabaseService,
    work_item_id: int,
    new_phase: Phase,
    gates_passed: List[str],
    artifacts: List[Dict[str, str]]
) -> WorkItem:
    """Record phase transition with history."""
    work_item = get_work_item(service, work_item_id)
    metadata = json.loads(work_item.metadata or '{}')

    # Initialize lifecycle if missing
    if 'lifecycle' not in metadata:
        metadata['lifecycle'] = {
            'phase_history': [],
            'status_history': [],
            'blockers': []
        }

    # Close previous phase
    phase_history = metadata['lifecycle']['phase_history']
    if phase_history:
        last_phase = phase_history[-1]
        if last_phase['exited_at'] is None:
            now = datetime.now().isoformat()
            last_phase['exited_at'] = now
            entered = datetime.fromisoformat(last_phase['entered_at'])
            exited = datetime.fromisoformat(now)
            last_phase['duration_hours'] = (exited - entered).total_seconds() / 3600

    # Add new phase entry
    phase_history.append({
        'phase': new_phase.value,
        'entered_at': datetime.now().isoformat(),
        'exited_at': None,
        'duration_hours': None,
        'artifacts': artifacts,
        'gates_passed': gates_passed
    })

    # Update indexed column + metadata
    return update_work_item(
        service,
        work_item_id,
        phase=new_phase,
        phase_entered_at=datetime.now(),
        metadata=json.dumps(metadata)
    )

def add_blocker(
    service: DatabaseService,
    work_item_id: int,
    reason: str
) -> WorkItem:
    """Add blocker with indexed tracking."""
    work_item = get_work_item(service, work_item_id)
    metadata = json.loads(work_item.metadata or '{}')

    if 'lifecycle' not in metadata:
        metadata['lifecycle'] = {'blockers': []}

    now = datetime.now()
    metadata['lifecycle']['blockers'].append({
        'reason': reason,
        'blocked_at': now.isoformat(),
        'resolved_at': None,
        'duration_hours': None,
        'resolution': None
    })

    return update_work_item(
        service,
        work_item_id,
        current_blocker_reason=reason,
        blocked_since=now,
        metadata=json.dumps(metadata)
    )

def resolve_blocker(
    service: DatabaseService,
    work_item_id: int,
    resolution: str
) -> WorkItem:
    """Resolve most recent blocker."""
    work_item = get_work_item(service, work_item_id)
    metadata = json.loads(work_item.metadata or '{}')

    blockers = metadata.get('lifecycle', {}).get('blockers', [])
    if blockers:
        last_blocker = blockers[-1]
        if last_blocker['resolved_at'] is None:
            now = datetime.now()
            last_blocker['resolved_at'] = now.isoformat()
            last_blocker['resolution'] = resolution

            blocked_since = datetime.fromisoformat(last_blocker['blocked_at'])
            last_blocker['duration_hours'] = (now - blocked_since).total_seconds() / 3600

    return update_work_item(
        service,
        work_item_id,
        current_blocker_reason=None,
        blocked_since=None,
        metadata=json.dumps(metadata)
    )

def update_quality_metrics(
    service: DatabaseService,
    work_item_id: int,
    test_coverage: Optional[float] = None,
    code_quality_score: Optional[float] = None,
    defects_found: Optional[int] = None
) -> WorkItem:
    """Update quality metrics with history."""
    work_item = get_work_item(service, work_item_id)
    metadata = json.loads(work_item.metadata or '{}')

    if 'quality_metrics' not in metadata:
        metadata['quality_metrics'] = {'test_coverage_history': []}

    # Track coverage evolution
    if test_coverage is not None:
        metadata['quality_metrics']['test_coverage_history'].append({
            'timestamp': datetime.now().isoformat(),
            'coverage': test_coverage
        })
        metadata['quality_metrics']['test_coverage'] = test_coverage

    # Update other metrics
    if code_quality_score is not None:
        metadata['quality_metrics']['code_quality_score'] = code_quality_score
    if defects_found is not None:
        metadata['quality_metrics']['defects_found'] = defects_found

    # Update indexed coverage column
    updates = {'metadata': json.dumps(metadata)}
    if test_coverage is not None:
        updates['test_coverage'] = test_coverage

    return update_work_item(service, work_item_id, **updates)
```

### Example Queries

```python
# Query 1: Find work items with low test coverage
def get_low_coverage_work_items(
    service: DatabaseService,
    threshold: float = 0.8
) -> List[WorkItem]:
    """Get work items below coverage threshold."""
    query = """
        SELECT * FROM work_items
        WHERE test_coverage < ?
        AND test_coverage IS NOT NULL
        ORDER BY test_coverage ASC
    """
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, (threshold,))
        rows = cursor.fetchall()
    return [WorkItemAdapter.from_db(dict(row)) for row in rows]

# Query 2: Get phase duration analytics
def get_phase_durations(
    service: DatabaseService,
    work_item_id: int
) -> Dict[str, float]:
    """Get duration spent in each phase."""
    work_item = get_work_item(service, work_item_id)
    metadata = json.loads(work_item.metadata or '{}')

    phase_history = metadata.get('lifecycle', {}).get('phase_history', [])
    durations = {}

    for entry in phase_history:
        if entry['duration_hours'] is not None:
            durations[entry['phase']] = entry['duration_hours']

    return durations

# Query 3: Get active blockers across project
def get_active_blockers(
    service: DatabaseService,
    project_id: int
) -> List[Dict[str, Any]]:
    """Get all active blockers in project."""
    query = """
        SELECT id, name, current_blocker_reason, blocked_since
        FROM work_items
        WHERE project_id = ?
        AND blocked_since IS NOT NULL
        ORDER BY blocked_since ASC
    """
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, (project_id,))
        return [dict(row) for row in cursor.fetchall()]

# Query 4: Get effort variance report
def get_effort_variance_report(
    service: DatabaseService,
    project_id: int
) -> List[Dict[str, Any]]:
    """Get effort variance for all work items."""
    work_items = list_work_items(service, project_id=project_id)

    report = []
    for wi in work_items:
        metadata = json.loads(wi.metadata or '{}')
        effort = metadata.get('effort_tracking', {})

        if effort.get('estimated_hours') and effort.get('actual_hours'):
            report.append({
                'id': wi.id,
                'name': wi.name,
                'estimated': effort['estimated_hours'],
                'actual': effort['actual_hours'],
                'variance_percent': effort['variance_percent']
            })

    return sorted(report, key=lambda x: abs(x['variance_percent']), reverse=True)
```

### Migration Strategy

```python
# Migration 0025: Add lifecycle tracking columns

def upgrade(service):
    """Add lifecycle tracking columns with indexes."""
    with service.transaction() as conn:
        # Add columns
        conn.execute("ALTER TABLE work_items ADD COLUMN phase_entered_at TIMESTAMP")
        conn.execute("ALTER TABLE work_items ADD COLUMN status_entered_at TIMESTAMP")
        conn.execute("ALTER TABLE work_items ADD COLUMN current_blocker_reason TEXT")
        conn.execute("ALTER TABLE work_items ADD COLUMN blocked_since TIMESTAMP")
        conn.execute("ALTER TABLE work_items ADD COLUMN test_coverage REAL")
        conn.execute("ALTER TABLE work_items ADD COLUMN actual_hours REAL")

        # Create indexes
        conn.execute("""
            CREATE INDEX idx_work_items_test_coverage
            ON work_items(test_coverage)
            WHERE test_coverage IS NOT NULL
        """)
        conn.execute("""
            CREATE INDEX idx_work_items_actual_hours
            ON work_items(actual_hours)
            WHERE actual_hours IS NOT NULL
        """)
        conn.execute("""
            CREATE INDEX idx_work_items_blocked
            ON work_items(blocked_since)
            WHERE blocked_since IS NOT NULL
        """)

        # Backfill timestamps for existing work items
        conn.execute("""
            UPDATE work_items
            SET phase_entered_at = created_at,
                status_entered_at = created_at
            WHERE phase_entered_at IS NULL
        """)

def downgrade(service):
    """Remove lifecycle tracking columns."""
    # SQLite doesn't support DROP COLUMN, would need table recreation
    # For production, implement full table recreation with data preservation
    pass
```

### Trade-offs

**✅ Advantages**:
1. **Fast Common Queries**: Indexed columns for coverage, blockers, effort
2. **Flexible Evolution**: JSON metadata for new metrics without migration
3. **Backward Compatible**: Existing queries unaffected, new columns optional
4. **Low Migration Risk**: Additive changes, no data restructuring
5. **Minimal Storage**: Only frequently-queried values duplicated

**❌ Disadvantages**:
1. **JSON Parsing Overhead**: Complex queries need JSON extraction
2. **Dual Maintenance**: Update both column + JSON for indexed fields
3. **Limited Historical Queries**: Can't efficiently filter by historical coverage
4. **Schema Inconsistency**: Some data in columns, some in JSON (developer confusion)
5. **Query Complexity**: Developers must know which data is where

**Performance**:
- Coverage filter: ~20ms (indexed)
- Phase duration: ~5ms (JSON parse)
- Active blockers: ~15ms (indexed)
- Historical analytics: ~100-200ms (JSON aggregation)

**Storage Overhead**:
- 6 new columns × 8 bytes average = 48 bytes per work item
- JSON metadata: ~2-5KB per work item (detailed history)
- Total: ~5KB per work item for full lifecycle data

---

## Option B: Separate Lifecycle Tables (Normalized)

### Architecture

**Philosophy**: Normalize lifecycle data into dedicated tables for optimal query performance and data integrity.

### Schema Changes

```sql
-- Phase history table
CREATE TABLE work_item_phase_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    work_item_id INTEGER NOT NULL,
    phase TEXT NOT NULL,
    entered_at TIMESTAMP NOT NULL,
    exited_at TIMESTAMP,
    duration_hours REAL,
    gates_passed TEXT,  -- JSON array of gate IDs
    artifacts TEXT,     -- JSON array of artifact references
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
);

CREATE INDEX idx_phase_history_work_item ON work_item_phase_history(work_item_id);
CREATE INDEX idx_phase_history_phase ON work_item_phase_history(phase);

-- Status history table
CREATE TABLE work_item_status_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    work_item_id INTEGER NOT NULL,
    status TEXT NOT NULL,
    entered_at TIMESTAMP NOT NULL,
    exited_at TIMESTAMP,
    duration_hours REAL,
    transition_reason TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
);

CREATE INDEX idx_status_history_work_item ON work_item_status_history(work_item_id);
CREATE INDEX idx_status_history_status ON work_item_status_history(status);

-- Blockers table
CREATE TABLE work_item_blockers (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    work_item_id INTEGER NOT NULL,
    reason TEXT NOT NULL,
    blocked_at TIMESTAMP NOT NULL,
    resolved_at TIMESTAMP,
    resolution TEXT,
    duration_hours REAL,
    blocker_type TEXT,  -- technical, approval, dependency, external
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
);

CREATE INDEX idx_blockers_work_item ON work_item_blockers(work_item_id);
CREATE INDEX idx_blockers_active ON work_item_blockers(resolved_at) WHERE resolved_at IS NULL;

-- Quality metrics table (snapshots)
CREATE TABLE work_item_quality_metrics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    work_item_id INTEGER NOT NULL,
    captured_at TIMESTAMP NOT NULL,
    test_coverage REAL,
    code_quality_score REAL,
    review_iterations INTEGER,
    defects_found INTEGER,
    defects_resolved INTEGER,
    static_analysis_violations INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
);

CREATE INDEX idx_quality_metrics_work_item ON work_item_quality_metrics(work_item_id);
CREATE INDEX idx_quality_metrics_coverage ON work_item_quality_metrics(test_coverage);
CREATE INDEX idx_quality_metrics_captured_at ON work_item_quality_metrics(captured_at);

-- Effort tracking table (time entries)
CREATE TABLE work_item_effort_entries (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    work_item_id INTEGER NOT NULL,
    entry_date DATE NOT NULL,
    hours REAL NOT NULL,
    phase TEXT,
    task_id INTEGER,
    agent_role TEXT,
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE,
    FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE SET NULL
);

CREATE INDEX idx_effort_entries_work_item ON work_item_effort_entries(work_item_id);
CREATE INDEX idx_effort_entries_date ON work_item_effort_entries(entry_date);

-- Decisions table
CREATE TABLE work_item_decisions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    work_item_id INTEGER NOT NULL,
    decision TEXT NOT NULL,
    rationale TEXT NOT NULL,
    alternatives_considered TEXT,  -- JSON array
    decision_maker TEXT,
    confidence REAL,  -- 0.0 to 1.0
    reversible BOOLEAN DEFAULT TRUE,
    timestamp TIMESTAMP NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
);

CREATE INDEX idx_decisions_work_item ON work_item_decisions(work_item_id);
CREATE INDEX idx_decisions_timestamp ON work_item_decisions(timestamp);
```

### Implementation

```python
# agentpm/core/database/methods/phase_history.py

def record_phase_entry(
    service: DatabaseService,
    work_item_id: int,
    phase: Phase,
    gates_passed: List[str],
    artifacts: List[Dict[str, str]]
) -> int:
    """Record phase entry."""
    query = """
        INSERT INTO work_item_phase_history
        (work_item_id, phase, entered_at, gates_passed, artifacts)
        VALUES (?, ?, ?, ?, ?)
    """
    with service.transaction() as conn:
        cursor = conn.execute(
            query,
            (
                work_item_id,
                phase.value,
                datetime.now(),
                json.dumps(gates_passed),
                json.dumps(artifacts)
            )
        )
        return cursor.lastrowid

def close_current_phase(
    service: DatabaseService,
    work_item_id: int
) -> None:
    """Close currently open phase entry."""
    query = """
        UPDATE work_item_phase_history
        SET exited_at = ?,
            duration_hours = (
                (julianday(?) - julianday(entered_at)) * 24
            )
        WHERE work_item_id = ?
        AND exited_at IS NULL
    """
    now = datetime.now()
    with service.transaction() as conn:
        conn.execute(query, (now, now, work_item_id))

def get_phase_history(
    service: DatabaseService,
    work_item_id: int
) -> List[Dict[str, Any]]:
    """Get complete phase history."""
    query = """
        SELECT phase, entered_at, exited_at, duration_hours,
               gates_passed, artifacts
        FROM work_item_phase_history
        WHERE work_item_id = ?
        ORDER BY entered_at ASC
    """
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, (work_item_id,))

        history = []
        for row in cursor.fetchall():
            entry = dict(row)
            entry['gates_passed'] = json.loads(entry['gates_passed'] or '[]')
            entry['artifacts'] = json.loads(entry['artifacts'] or '[]')
            history.append(entry)

        return history

# agentpm/core/database/methods/blockers.py

def add_blocker(
    service: DatabaseService,
    work_item_id: int,
    reason: str,
    blocker_type: str = 'technical'
) -> int:
    """Add blocker record."""
    query = """
        INSERT INTO work_item_blockers
        (work_item_id, reason, blocked_at, blocker_type)
        VALUES (?, ?, ?, ?)
    """
    with service.transaction() as conn:
        cursor = conn.execute(
            query,
            (work_item_id, reason, datetime.now(), blocker_type)
        )
        return cursor.lastrowid

def resolve_blocker(
    service: DatabaseService,
    blocker_id: int,
    resolution: str
) -> None:
    """Resolve specific blocker."""
    query = """
        UPDATE work_item_blockers
        SET resolved_at = ?,
            resolution = ?,
            duration_hours = (
                (julianday(?) - julianday(blocked_at)) * 24
            )
        WHERE id = ?
    """
    now = datetime.now()
    with service.transaction() as conn:
        conn.execute(query, (now, resolution, now, blocker_id))

def get_active_blockers(
    service: DatabaseService,
    project_id: Optional[int] = None
) -> List[Dict[str, Any]]:
    """Get all active blockers."""
    if project_id:
        query = """
            SELECT b.*, wi.name as work_item_name
            FROM work_item_blockers b
            JOIN work_items wi ON b.work_item_id = wi.id
            WHERE wi.project_id = ?
            AND b.resolved_at IS NULL
            ORDER BY b.blocked_at ASC
        """
        params = (project_id,)
    else:
        query = """
            SELECT b.*, wi.name as work_item_name
            FROM work_item_blockers b
            JOIN work_items wi ON b.work_item_id = wi.id
            WHERE b.resolved_at IS NULL
            ORDER BY b.blocked_at ASC
        """
        params = ()

    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, params)
        return [dict(row) for row in cursor.fetchall()]

# agentpm/core/database/methods/quality_metrics.py

def record_quality_snapshot(
    service: DatabaseService,
    work_item_id: int,
    test_coverage: Optional[float] = None,
    code_quality_score: Optional[float] = None,
    review_iterations: Optional[int] = None,
    defects_found: Optional[int] = None,
    defects_resolved: Optional[int] = None
) -> int:
    """Record quality metrics snapshot."""
    query = """
        INSERT INTO work_item_quality_metrics
        (work_item_id, captured_at, test_coverage, code_quality_score,
         review_iterations, defects_found, defects_resolved)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    """
    with service.transaction() as conn:
        cursor = conn.execute(
            query,
            (
                work_item_id,
                datetime.now(),
                test_coverage,
                code_quality_score,
                review_iterations,
                defects_found,
                defects_resolved
            )
        )
        return cursor.lastrowid

def get_latest_quality_metrics(
    service: DatabaseService,
    work_item_id: int
) -> Optional[Dict[str, Any]]:
    """Get most recent quality metrics."""
    query = """
        SELECT * FROM work_item_quality_metrics
        WHERE work_item_id = ?
        ORDER BY captured_at DESC
        LIMIT 1
    """
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, (work_item_id,))
        row = cursor.fetchone()
        return dict(row) if row else None

def get_coverage_evolution(
    service: DatabaseService,
    work_item_id: int
) -> List[Dict[str, Any]]:
    """Get test coverage evolution over time."""
    query = """
        SELECT captured_at, test_coverage
        FROM work_item_quality_metrics
        WHERE work_item_id = ?
        AND test_coverage IS NOT NULL
        ORDER BY captured_at ASC
    """
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, (work_item_id,))
        return [dict(row) for row in cursor.fetchall()]
```

### Example Queries

```python
# Query 1: Find work items with low coverage (using latest snapshot)
def get_low_coverage_work_items(
    service: DatabaseService,
    threshold: float = 0.8
) -> List[Dict[str, Any]]:
    """Get work items below coverage threshold."""
    query = """
        SELECT wi.id, wi.name, qm.test_coverage
        FROM work_items wi
        JOIN (
            SELECT work_item_id, MAX(captured_at) as latest
            FROM work_item_quality_metrics
            GROUP BY work_item_id
        ) latest_qm ON wi.id = latest_qm.work_item_id
        JOIN work_item_quality_metrics qm
            ON qm.work_item_id = latest_qm.work_item_id
            AND qm.captured_at = latest_qm.latest
        WHERE qm.test_coverage < ?
        ORDER BY qm.test_coverage ASC
    """
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, (threshold,))
        return [dict(row) for row in cursor.fetchall()]

# Query 2: Get average phase durations across project
def get_average_phase_durations(
    service: DatabaseService,
    project_id: int
) -> Dict[str, float]:
    """Get average time spent in each phase."""
    query = """
        SELECT ph.phase, AVG(ph.duration_hours) as avg_duration
        FROM work_item_phase_history ph
        JOIN work_items wi ON ph.work_item_id = wi.id
        WHERE wi.project_id = ?
        AND ph.duration_hours IS NOT NULL
        GROUP BY ph.phase
    """
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, (project_id,))
        return {row['phase']: row['avg_duration'] for row in cursor.fetchall()}

# Query 3: Get blocker statistics
def get_blocker_statistics(
    service: DatabaseService,
    project_id: int
) -> Dict[str, Any]:
    """Get blocker statistics for project."""
    query = """
        SELECT
            COUNT(*) as total_blockers,
            COUNT(CASE WHEN resolved_at IS NULL THEN 1 END) as active_blockers,
            AVG(duration_hours) as avg_resolution_hours,
            blocker_type,
            COUNT(*) as count_by_type
        FROM work_item_blockers b
        JOIN work_items wi ON b.work_item_id = wi.id
        WHERE wi.project_id = ?
        GROUP BY blocker_type
    """
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, (project_id,))
        return [dict(row) for row in cursor.fetchall()]

# Query 4: Get effort variance report
def get_effort_variance_report(
    service: DatabaseService,
    project_id: int
) -> List[Dict[str, Any]]:
    """Get effort variance analysis."""
    query = """
        SELECT
            wi.id,
            wi.name,
            wi.effort_estimate_hours as estimated,
            SUM(ee.hours) as actual,
            ((SUM(ee.hours) - wi.effort_estimate_hours) / wi.effort_estimate_hours * 100) as variance_percent
        FROM work_items wi
        JOIN work_item_effort_entries ee ON wi.id = ee.work_item_id
        WHERE wi.project_id = ?
        AND wi.effort_estimate_hours IS NOT NULL
        GROUP BY wi.id, wi.name, wi.effort_estimate_hours
        ORDER BY ABS(variance_percent) DESC
    """
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, (project_id,))
        return [dict(row) for row in cursor.fetchall()]
```

### Migration Strategy

```python
# Migration 0025: Create lifecycle tracking tables

def upgrade(service):
    """Create normalized lifecycle tables."""
    with service.transaction() as conn:
        # Phase history
        conn.execute("""
            CREATE TABLE work_item_phase_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                work_item_id INTEGER NOT NULL,
                phase TEXT NOT NULL,
                entered_at TIMESTAMP NOT NULL,
                exited_at TIMESTAMP,
                duration_hours REAL,
                gates_passed TEXT,
                artifacts TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
            )
        """)
        conn.execute("CREATE INDEX idx_phase_history_work_item ON work_item_phase_history(work_item_id)")

        # Status history
        conn.execute("""
            CREATE TABLE work_item_status_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                work_item_id INTEGER NOT NULL,
                status TEXT NOT NULL,
                entered_at TIMESTAMP NOT NULL,
                exited_at TIMESTAMP,
                duration_hours REAL,
                transition_reason TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
            )
        """)
        conn.execute("CREATE INDEX idx_status_history_work_item ON work_item_status_history(work_item_id)")

        # Blockers
        conn.execute("""
            CREATE TABLE work_item_blockers (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                work_item_id INTEGER NOT NULL,
                reason TEXT NOT NULL,
                blocked_at TIMESTAMP NOT NULL,
                resolved_at TIMESTAMP,
                resolution TEXT,
                duration_hours REAL,
                blocker_type TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
            )
        """)
        conn.execute("CREATE INDEX idx_blockers_work_item ON work_item_blockers(work_item_id)")
        conn.execute("CREATE INDEX idx_blockers_active ON work_item_blockers(resolved_at) WHERE resolved_at IS NULL")

        # Quality metrics
        conn.execute("""
            CREATE TABLE work_item_quality_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                work_item_id INTEGER NOT NULL,
                captured_at TIMESTAMP NOT NULL,
                test_coverage REAL,
                code_quality_score REAL,
                review_iterations INTEGER,
                defects_found INTEGER,
                defects_resolved INTEGER,
                static_analysis_violations INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
            )
        """)
        conn.execute("CREATE INDEX idx_quality_metrics_work_item ON work_item_quality_metrics(work_item_id)")
        conn.execute("CREATE INDEX idx_quality_metrics_coverage ON work_item_quality_metrics(test_coverage)")

        # Effort entries
        conn.execute("""
            CREATE TABLE work_item_effort_entries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                work_item_id INTEGER NOT NULL,
                entry_date DATE NOT NULL,
                hours REAL NOT NULL,
                phase TEXT,
                task_id INTEGER,
                agent_role TEXT,
                notes TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE,
                FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE SET NULL
            )
        """)
        conn.execute("CREATE INDEX idx_effort_entries_work_item ON work_item_effort_entries(work_item_id)")

        # Decisions
        conn.execute("""
            CREATE TABLE work_item_decisions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                work_item_id INTEGER NOT NULL,
                decision TEXT NOT NULL,
                rationale TEXT NOT NULL,
                alternatives_considered TEXT,
                decision_maker TEXT,
                confidence REAL,
                reversible BOOLEAN DEFAULT TRUE,
                timestamp TIMESTAMP NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
            )
        """)
        conn.execute("CREATE INDEX idx_decisions_work_item ON work_item_decisions(work_item_id)")

        # Backfill initial phase/status entries for existing work items
        conn.execute("""
            INSERT INTO work_item_phase_history (work_item_id, phase, entered_at)
            SELECT id, phase, created_at
            FROM work_items
            WHERE phase IS NOT NULL
        """)

        conn.execute("""
            INSERT INTO work_item_status_history (work_item_id, status, entered_at)
            SELECT id, status, created_at
            FROM work_items
        """)

def downgrade(service):
    """Remove lifecycle tables."""
    with service.transaction() as conn:
        conn.execute("DROP TABLE IF EXISTS work_item_decisions")
        conn.execute("DROP TABLE IF EXISTS work_item_effort_entries")
        conn.execute("DROP TABLE IF EXISTS work_item_quality_metrics")
        conn.execute("DROP TABLE IF EXISTS work_item_blockers")
        conn.execute("DROP TABLE IF EXISTS work_item_status_history")
        conn.execute("DROP TABLE IF EXISTS work_item_phase_history")
```

### Trade-offs

**✅ Advantages**:
1. **Optimal Query Performance**: All queries use indexed joins, no JSON parsing
2. **Rich Historical Queries**: Filter by any metric at any point in time
3. **Data Integrity**: Foreign key constraints, type checking, normalization
4. **Flexible Analytics**: Easy aggregations (AVG phase duration, blocker stats)
5. **Clear Schema**: Explicit structure, no guessing what's in JSON

**❌ Disadvantages**:
1. **Higher Migration Complexity**: 6 new tables, careful backfill required
2. **More Storage Overhead**: Normalized data duplication, more rows
3. **Query Complexity**: Joins required for most queries (vs simple SELECT)
4. **Rigidity**: Schema changes require migrations (vs JSON flexibility)
5. **Development Overhead**: More tables to maintain, more CRUD methods

**Performance**:
- Coverage filter: ~10ms (optimized join with index)
- Phase duration: ~5ms (indexed query)
- Active blockers: ~8ms (indexed WHERE clause)
- Historical analytics: ~20-50ms (aggregations with indexes)

**Storage Overhead**:
- Per work item with 5 phases, 2 blockers, 10 quality snapshots, 20 effort entries:
  - Phase history: 5 rows × 100 bytes = 500 bytes
  - Status history: 5 rows × 80 bytes = 400 bytes
  - Blockers: 2 rows × 120 bytes = 240 bytes
  - Quality metrics: 10 rows × 60 bytes = 600 bytes
  - Effort entries: 20 rows × 80 bytes = 1600 bytes
  - Decisions: ~2 rows × 200 bytes = 400 bytes
  - **Total: ~3.7KB per work item** (lower than JSON if detailed history)

---

## Option C: Hybrid (Structured Fields + Flexible Metadata)

### Architecture

**Philosophy**: Balance between query performance and flexibility. Use structured fields for critical metrics, JSON metadata for evolving/optional data.

### Schema Changes

```sql
-- Add critical lifecycle fields (frequently queried)
ALTER TABLE work_items ADD COLUMN phase_entered_at TIMESTAMP;
ALTER TABLE work_items ADD COLUMN status_entered_at TIMESTAMP;
ALTER TABLE work_items ADD COLUMN test_coverage REAL;  -- Latest coverage
ALTER TABLE work_items ADD COLUMN actual_hours REAL;   -- Total actual effort
ALTER TABLE work_items ADD COLUMN active_blocker_count INTEGER DEFAULT 0;

-- Create lightweight history tables (only for audit/reporting)
CREATE TABLE work_item_phase_transitions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    work_item_id INTEGER NOT NULL,
    from_phase TEXT,
    to_phase TEXT NOT NULL,
    transitioned_at TIMESTAMP NOT NULL,
    duration_in_from_phase_hours REAL,
    gates_passed TEXT,  -- JSON array
    FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
);

CREATE INDEX idx_phase_transitions_work_item ON work_item_phase_transitions(work_item_id);

CREATE TABLE work_item_blockers (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    work_item_id INTEGER NOT NULL,
    reason TEXT NOT NULL,
    blocked_at TIMESTAMP NOT NULL,
    resolved_at TIMESTAMP,
    resolution TEXT,
    FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
);

CREATE INDEX idx_blockers_work_item ON work_item_blockers(work_item_id);
CREATE INDEX idx_blockers_active ON work_item_blockers(resolved_at) WHERE resolved_at IS NULL;

-- Create indexes for critical fields
CREATE INDEX idx_work_items_test_coverage ON work_items(test_coverage) WHERE test_coverage IS NOT NULL;
CREATE INDEX idx_work_items_actual_hours ON work_items(actual_hours) WHERE actual_hours IS NOT NULL;
CREATE INDEX idx_work_items_active_blockers ON work_items(active_blocker_count) WHERE active_blocker_count > 0;
```

### Enhanced Metadata JSON (Supplementary Data)

```json
{
  "quality_metrics": {
    "coverage_history": [
      {"timestamp": "2025-10-17T14:00:00Z", "coverage": 0.78},
      {"timestamp": "2025-10-17T16:00:00Z", "coverage": 0.92}
    ],
    "code_quality_score": 0.88,
    "review_iterations": 2,
    "defects": {"found": 3, "resolved": 3},
    "static_analysis": {"complexity": "moderate", "violations": 0}
  },
  "effort_tracking": {
    "time_entries": [
      {"date": "2025-10-16", "hours": 8.0, "phase": "D1_DISCOVERY"},
      {"date": "2025-10-17", "hours": 7.5, "phase": "P1_PLAN"}
    ],
    "estimated_hours": 40.0,
    "variance_percent": 13.75
  },
  "decisions": [
    {
      "decision": "Use PostgreSQL",
      "rationale": "Better JSON support, ACID guarantees",
      "alternatives": ["MySQL", "MongoDB"],
      "timestamp": "2025-10-16T12:00:00Z",
      "decision_maker": "aipm-database-developer"
    }
  ],
  "artifacts": {
    "6w_analysis": ".agentpm/artifacts/wi_60_6w.md",
    "design_doc": ".agentpm/artifacts/wi_60_design.md"
  }
}
```

### Implementation

```python
# agentpm/core/database/methods/work_items.py

def transition_phase(
    service: DatabaseService,
    work_item_id: int,
    new_phase: Phase,
    gates_passed: List[str]
) -> WorkItem:
    """Transition to new phase with history tracking."""
    work_item = get_work_item(service, work_item_id)

    # Calculate duration in previous phase
    duration_hours = None
    if work_item.phase_entered_at:
        duration = datetime.now() - work_item.phase_entered_at
        duration_hours = duration.total_seconds() / 3600

    # Record transition in history table
    with service.transaction() as conn:
        conn.execute("""
            INSERT INTO work_item_phase_transitions
            (work_item_id, from_phase, to_phase, transitioned_at,
             duration_in_from_phase_hours, gates_passed)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            work_item_id,
            work_item.phase.value if work_item.phase else None,
            new_phase.value,
            datetime.now(),
            duration_hours,
            json.dumps(gates_passed)
        ))

    # Update work item current phase
    return update_work_item(
        service,
        work_item_id,
        phase=new_phase,
        phase_entered_at=datetime.now()
    )

def add_blocker(
    service: DatabaseService,
    work_item_id: int,
    reason: str
) -> int:
    """Add blocker and update count."""
    query = """
        INSERT INTO work_item_blockers (work_item_id, reason, blocked_at)
        VALUES (?, ?, ?)
    """

    with service.transaction() as conn:
        cursor = conn.execute(query, (work_item_id, reason, datetime.now()))
        blocker_id = cursor.lastrowid

        # Update active blocker count
        conn.execute("""
            UPDATE work_items
            SET active_blocker_count = (
                SELECT COUNT(*) FROM work_item_blockers
                WHERE work_item_id = ? AND resolved_at IS NULL
            )
            WHERE id = ?
        """, (work_item_id, work_item_id))

    return blocker_id

def resolve_blocker(
    service: DatabaseService,
    blocker_id: int,
    resolution: str
) -> None:
    """Resolve blocker and update count."""
    # Get blocker to find work_item_id
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(
            "SELECT work_item_id FROM work_item_blockers WHERE id = ?",
            (blocker_id,)
        )
        row = cursor.fetchone()
        if not row:
            return
        work_item_id = row['work_item_id']

    # Resolve blocker
    with service.transaction() as conn:
        conn.execute("""
            UPDATE work_item_blockers
            SET resolved_at = ?, resolution = ?
            WHERE id = ?
        """, (datetime.now(), resolution, blocker_id))

        # Update active blocker count
        conn.execute("""
            UPDATE work_items
            SET active_blocker_count = (
                SELECT COUNT(*) FROM work_item_blockers
                WHERE work_item_id = ? AND resolved_at IS NULL
            )
            WHERE id = ?
        """, (work_item_id, work_item_id))

def update_quality_metrics(
    service: DatabaseService,
    work_item_id: int,
    test_coverage: Optional[float] = None,
    code_quality_score: Optional[float] = None
) -> WorkItem:
    """Update quality metrics (field + metadata)."""
    work_item = get_work_item(service, work_item_id)
    metadata = json.loads(work_item.metadata or '{}')

    # Initialize quality_metrics in metadata
    if 'quality_metrics' not in metadata:
        metadata['quality_metrics'] = {'coverage_history': []}

    # Update coverage history in metadata
    if test_coverage is not None:
        metadata['quality_metrics']['coverage_history'].append({
            'timestamp': datetime.now().isoformat(),
            'coverage': test_coverage
        })

    # Update code quality score in metadata
    if code_quality_score is not None:
        metadata['quality_metrics']['code_quality_score'] = code_quality_score

    # Update database (field + metadata)
    updates = {'metadata': json.dumps(metadata)}
    if test_coverage is not None:
        updates['test_coverage'] = test_coverage  # Indexed field

    return update_work_item(service, work_item_id, **updates)

def track_effort_entry(
    service: DatabaseService,
    work_item_id: int,
    hours: float,
    date: Optional[datetime] = None,
    phase: Optional[Phase] = None
) -> WorkItem:
    """Track effort entry and update total."""
    work_item = get_work_item(service, work_item_id)
    metadata = json.loads(work_item.metadata or '{}')

    # Initialize effort_tracking in metadata
    if 'effort_tracking' not in metadata:
        metadata['effort_tracking'] = {
            'time_entries': [],
            'estimated_hours': work_item.effort_estimate_hours
        }

    # Add time entry
    entry_date = date or datetime.now()
    metadata['effort_tracking']['time_entries'].append({
        'date': entry_date.date().isoformat(),
        'hours': hours,
        'phase': phase.value if phase else work_item.phase.value if work_item.phase else None
    })

    # Calculate total actual hours
    total_actual = sum(e['hours'] for e in metadata['effort_tracking']['time_entries'])

    # Calculate variance
    if work_item.effort_estimate_hours:
        variance = ((total_actual - work_item.effort_estimate_hours) /
                   work_item.effort_estimate_hours * 100)
        metadata['effort_tracking']['variance_percent'] = round(variance, 2)

    # Update database (field + metadata)
    return update_work_item(
        service,
        work_item_id,
        actual_hours=total_actual,
        metadata=json.dumps(metadata)
    )
```

### Example Queries

```python
# Query 1: Find work items with low coverage (fast - indexed)
def get_low_coverage_work_items(
    service: DatabaseService,
    threshold: float = 0.8
) -> List[WorkItem]:
    """Get work items below coverage threshold."""
    query = """
        SELECT * FROM work_items
        WHERE test_coverage < ?
        AND test_coverage IS NOT NULL
        ORDER BY test_coverage ASC
    """
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, (threshold,))
        rows = cursor.fetchall()
    return [WorkItemAdapter.from_db(dict(row)) for row in rows]

# Query 2: Get phase transition history (fast - table query)
def get_phase_transitions(
    service: DatabaseService,
    work_item_id: int
) -> List[Dict[str, Any]]:
    """Get phase transition history."""
    query = """
        SELECT from_phase, to_phase, transitioned_at,
               duration_in_from_phase_hours, gates_passed
        FROM work_item_phase_transitions
        WHERE work_item_id = ?
        ORDER BY transitioned_at ASC
    """
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, (work_item_id,))

        transitions = []
        for row in cursor.fetchall():
            transition = dict(row)
            transition['gates_passed'] = json.loads(transition['gates_passed'] or '[]')
            transitions.append(transition)

        return transitions

# Query 3: Get active blockers (fast - indexed)
def get_active_blockers(
    service: DatabaseService,
    project_id: int
) -> List[Dict[str, Any]]:
    """Get all active blockers in project."""
    query = """
        SELECT
            wi.id as work_item_id,
            wi.name as work_item_name,
            wi.active_blocker_count,
            b.id as blocker_id,
            b.reason,
            b.blocked_at
        FROM work_items wi
        JOIN work_item_blockers b ON wi.id = b.work_item_id
        WHERE wi.project_id = ?
        AND b.resolved_at IS NULL
        ORDER BY b.blocked_at ASC
    """
    with service.connect() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query, (project_id,))
        return [dict(row) for row in cursor.fetchall()]

# Query 4: Get effort variance report (indexed + JSON parse)
def get_effort_variance_report(
    service: DatabaseService,
    project_id: int
) -> List[Dict[str, Any]]:
    """Get effort variance analysis."""
    work_items = list_work_items(service, project_id=project_id)

    report = []
    for wi in work_items:
        if not wi.effort_estimate_hours or not wi.actual_hours:
            continue

        metadata = json.loads(wi.metadata or '{}')
        effort = metadata.get('effort_tracking', {})

        report.append({
            'id': wi.id,
            'name': wi.name,
            'estimated': wi.effort_estimate_hours,
            'actual': wi.actual_hours,  # From indexed field
            'variance_percent': effort.get('variance_percent', 0)
        })

    return sorted(report, key=lambda x: abs(x['variance_percent']), reverse=True)

# Query 5: Get coverage evolution (JSON parse, not frequently used)
def get_coverage_evolution(
    service: DatabaseService,
    work_item_id: int
) -> List[Dict[str, float]]:
    """Get coverage evolution from metadata."""
    work_item = get_work_item(service, work_item_id)
    metadata = json.loads(work_item.metadata or '{}')

    return metadata.get('quality_metrics', {}).get('coverage_history', [])
```

### Migration Strategy

```python
# Migration 0025: Add hybrid lifecycle tracking

def upgrade(service):
    """Add hybrid lifecycle fields and tables."""
    with service.transaction() as conn:
        # Add indexed fields to work_items
        conn.execute("ALTER TABLE work_items ADD COLUMN phase_entered_at TIMESTAMP")
        conn.execute("ALTER TABLE work_items ADD COLUMN status_entered_at TIMESTAMP")
        conn.execute("ALTER TABLE work_items ADD COLUMN test_coverage REAL")
        conn.execute("ALTER TABLE work_items ADD COLUMN actual_hours REAL")
        conn.execute("ALTER TABLE work_items ADD COLUMN active_blocker_count INTEGER DEFAULT 0")

        # Create indexes
        conn.execute("""
            CREATE INDEX idx_work_items_test_coverage
            ON work_items(test_coverage)
            WHERE test_coverage IS NOT NULL
        """)
        conn.execute("""
            CREATE INDEX idx_work_items_actual_hours
            ON work_items(actual_hours)
            WHERE actual_hours IS NOT NULL
        """)
        conn.execute("""
            CREATE INDEX idx_work_items_active_blockers
            ON work_items(active_blocker_count)
            WHERE active_blocker_count > 0
        """)

        # Create lightweight history tables
        conn.execute("""
            CREATE TABLE work_item_phase_transitions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                work_item_id INTEGER NOT NULL,
                from_phase TEXT,
                to_phase TEXT NOT NULL,
                transitioned_at TIMESTAMP NOT NULL,
                duration_in_from_phase_hours REAL,
                gates_passed TEXT,
                FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
            )
        """)
        conn.execute("CREATE INDEX idx_phase_transitions_work_item ON work_item_phase_transitions(work_item_id)")

        conn.execute("""
            CREATE TABLE work_item_blockers (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                work_item_id INTEGER NOT NULL,
                reason TEXT NOT NULL,
                blocked_at TIMESTAMP NOT NULL,
                resolved_at TIMESTAMP,
                resolution TEXT,
                FOREIGN KEY (work_item_id) REFERENCES work_items(id) ON DELETE CASCADE
            )
        """)
        conn.execute("CREATE INDEX idx_blockers_work_item ON work_item_blockers(work_item_id)")
        conn.execute("CREATE INDEX idx_blockers_active ON work_item_blockers(resolved_at) WHERE resolved_at IS NULL")

        # Backfill timestamps
        conn.execute("""
            UPDATE work_items
            SET phase_entered_at = created_at,
                status_entered_at = created_at
            WHERE phase_entered_at IS NULL
        """)

        # Backfill initial phase transitions
        conn.execute("""
            INSERT INTO work_item_phase_transitions (work_item_id, to_phase, transitioned_at)
            SELECT id, phase, created_at
            FROM work_items
            WHERE phase IS NOT NULL
        """)

def downgrade(service):
    """Remove hybrid lifecycle tracking."""
    with service.transaction() as conn:
        conn.execute("DROP TABLE IF EXISTS work_item_blockers")
        conn.execute("DROP TABLE IF EXISTS work_item_phase_transitions")
        # Note: SQLite doesn't support DROP COLUMN easily
        # In production, would need table recreation for full downgrade
```

### Trade-offs

**✅ Advantages**:
1. **Fast Critical Queries**: Indexed fields for coverage, effort, blockers
2. **Flexible Evolution**: JSON metadata for detailed/optional data
3. **Balanced Storage**: Minimal table overhead, JSON for rich history
4. **Moderate Complexity**: 2 new tables vs 6 (Option B)
5. **Clear Priorities**: Developers know what's indexed vs metadata

**❌ Disadvantages**:
1. **Split Brain**: Some data in fields, some in metadata (consistency risk)
2. **Limited Historical Queries**: Can't filter by historical coverage efficiently
3. **Dual Maintenance**: Critical fields updated in both places
4. **Schema Evolution**: Adding new critical fields requires migration

**Performance**:
- Coverage filter: ~20ms (indexed field)
- Phase transitions: ~10ms (table query)
- Active blockers: ~15ms (indexed join)
- Effort variance: ~30ms (indexed field + JSON parse)
- Coverage evolution: ~5ms (JSON parse, single work item)

**Storage Overhead**:
- Indexed fields: 5 columns × 8 bytes = 40 bytes per work item
- Phase transitions: ~5 rows × 80 bytes = 400 bytes
- Blockers: ~2 rows × 100 bytes = 200 bytes
- JSON metadata: ~1-3KB (supplementary data only)
- **Total: ~3.6KB per work item**

---

## Comparative Analysis

### Query Performance Matrix

| Query Type | Option A (JSON) | Option B (Tables) | Option C (Hybrid) |
|------------|-----------------|-------------------|-------------------|
| Filter by coverage | 20ms (indexed) | 10ms (join+index) | 20ms (indexed) |
| Get phase history | 5ms (JSON parse) | 5ms (table query) | 10ms (table query) |
| Active blockers | 15ms (indexed) | 8ms (indexed join) | 15ms (indexed join) |
| Effort variance | 100ms (JSON agg) | 20ms (agg+join) | 30ms (field+JSON) |
| Historical coverage | 200ms (JSON scan) | 50ms (indexed scan) | 5ms (JSON parse) |
| Average phase duration | 150ms (JSON agg) | 20ms (AVG query) | 50ms (AVG query) |
| Blocker statistics | 100ms (JSON parse) | 15ms (GROUP BY) | 25ms (GROUP BY) |

### Storage Overhead Comparison

| Metric | Option A | Option B | Option C |
|--------|----------|----------|----------|
| Work item fields | 48 bytes | 0 bytes | 40 bytes |
| JSON metadata | 2-5 KB | ~500 bytes | 1-3 KB |
| History tables | 0 rows | ~45 rows/item | ~7 rows/item |
| **Total per item** | **~5 KB** | **~3.7 KB** | **~3.6 KB** |

### Migration Complexity Score

| Factor | Option A | Option B | Option C |
|--------|----------|----------|----------|
| New tables | 0 | 6 | 2 |
| New columns | 6 | 0 | 5 |
| Indexes | 3 | 12 | 5 |
| Backfill logic | Simple | Complex | Moderate |
| Rollback difficulty | Low | High | Moderate |
| **Overall Complexity** | **Low** | **High** | **Medium** |

### Development Maintenance Score

| Factor | Option A | Option B | Option C |
|--------|----------|----------|----------|
| CRUD methods | 5 | 15 | 10 |
| Query complexity | Medium | High (joins) | Medium |
| Schema evolution | Easy (JSON) | Hard (migrations) | Moderate |
| Data consistency | Medium (JSON sync) | High (constraints) | Medium (dual update) |
| Developer confusion | Medium (where's data?) | Low (explicit) | Medium (split) |
| **Overall Maintenance** | **Medium** | **High** | **Medium** |

### Flexibility Score

| Capability | Option A | Option B | Option C |
|------------|----------|----------|----------|
| Add new metrics | Easy (JSON) | Hard (migration) | Moderate (depends) |
| Historical queries | Poor (JSON scan) | Excellent (indexes) | Good (tables+JSON) |
| Complex analytics | Poor (JSON agg) | Excellent (SQL agg) | Good (depends) |
| Schema evolution | Excellent (JSON) | Poor (migrations) | Good (hybrid) |
| **Overall Flexibility** | **Good** | **Medium** | **Good** |

---

## Recommendation: Option C (Hybrid Approach)

### Rationale

After comprehensive evaluation, **Option C (Hybrid)** provides the optimal balance for AIPM's requirements:

**1. Performance Where It Matters**
- Critical filters (coverage, blockers, effort) use indexed fields: ~15-20ms
- Phase transitions tracked in dedicated table for audit/reporting
- Acceptable performance for all common queries (<50ms)

**2. Flexibility for Evolution**
- JSON metadata for quality history, decisions, artifacts
- Easy to add new metrics without breaking changes
- Supports experimentation with new quality frameworks

**3. Balanced Complexity**
- 2 new tables vs 6 (Option B) = less migration risk
- 5 new indexed columns = faster common queries vs Option A
- Moderate CRUD overhead vs Option B's 15 methods

**4. Storage Efficiency**
- ~3.6KB per work item (similar to Option B)
- Only 2 lightweight history tables (phase transitions, blockers)
- JSON metadata contains supplementary data only

**5. Developer Experience**
- Clear pattern: Indexed fields for queries, JSON for details
- Moderate learning curve (vs Option B's many tables)
- Explicit schema for critical data (vs Option A's hidden JSON)

### Implementation Priority

**Phase 1: Core Tracking (Week 1-2)**
- Migration 0025: Add indexed fields + history tables
- Implement phase transition tracking
- Implement blocker tracking
- Add test coverage field updates

**Phase 2: Quality Metrics (Week 3)**
- Implement quality metrics tracking
- Add coverage evolution to metadata
- Create quality snapshot methods

**Phase 3: Effort Tracking (Week 4)**
- Implement effort entry tracking
- Add variance calculations
- Create effort variance reports

**Phase 4: Decision Recording (Week 5)**
- Implement decision recording in metadata
- Create decision query methods
- Add decision display to CLI

**Phase 5: Analytics & Reporting (Week 6)**
- Build dashboard queries
- Create analytics methods
- Add CLI reporting commands

### Migration Risk Mitigation

**Backward Compatibility**
- All new fields are optional (NULL allowed)
- Existing queries continue to work
- Metadata JSON is additive (no breaking changes)

**Rollback Strategy**
- Drop new tables (history preserved in backups)
- SQLite table recreation for column removal (complex but possible)
- JSON metadata rollback: remove new keys

**Data Validation**
- Validate indexed field = JSON field consistency
- Add database triggers for automatic sync
- Periodic audit job to detect inconsistencies

---

## Next Steps

### Immediate Actions

1. **Review & Approval**
   - Architecture review with team
   - Validate trade-offs align with requirements
   - Confirm migration strategy

2. **Prototype Development**
   - Implement Migration 0025
   - Build core tracking methods
   - Create example queries

3. **Testing Strategy**
   - Unit tests for CRUD methods
   - Integration tests for queries
   - Performance benchmarks

4. **Documentation**
   - Developer guide for lifecycle tracking
   - API reference for new methods
   - Migration guide for existing projects

### Success Metrics

**Performance Targets**:
- Coverage filter: <25ms (99th percentile)
- Phase history: <15ms
- Blocker queries: <20ms
- Effort variance: <50ms

**Quality Targets**:
- Zero data loss during migration
- <5% storage overhead vs current
- 100% backward compatibility

**Developer Experience**:
- <30 minutes to understand new APIs
- Clear documentation with examples
- Consistent patterns with existing code

---

## Appendix: SQL Examples

### Option C: Complete Implementation Examples

```sql
-- Example 1: Get work items with coverage < 80% (indexed)
SELECT id, name, test_coverage
FROM work_items
WHERE test_coverage < 0.8
AND test_coverage IS NOT NULL
ORDER BY test_coverage ASC;

-- Example 2: Get phase transition timeline
SELECT
    wip.to_phase,
    wip.transitioned_at,
    wip.duration_in_from_phase_hours,
    json_extract(wip.gates_passed, '$') as gates
FROM work_item_phase_transitions wip
WHERE wip.work_item_id = 60
ORDER BY wip.transitioned_at ASC;

-- Example 3: Get blocker resolution time statistics
SELECT
    AVG(CAST((julianday(resolved_at) - julianday(blocked_at)) * 24 AS REAL)) as avg_hours,
    MIN(CAST((julianday(resolved_at) - julianday(blocked_at)) * 24 AS REAL)) as min_hours,
    MAX(CAST((julianday(resolved_at) - julianday(blocked_at)) * 24 AS REAL)) as max_hours
FROM work_item_blockers
WHERE work_item_id = 60
AND resolved_at IS NOT NULL;

-- Example 4: Get effort variance for project (indexed + metadata)
SELECT
    wi.id,
    wi.name,
    wi.effort_estimate_hours as estimated,
    wi.actual_hours as actual,
    CAST(((wi.actual_hours - wi.effort_estimate_hours) / wi.effort_estimate_hours * 100) AS REAL) as variance_percent
FROM work_items wi
WHERE wi.project_id = 1
AND wi.effort_estimate_hours IS NOT NULL
AND wi.actual_hours IS NOT NULL
ORDER BY ABS(variance_percent) DESC;

-- Example 5: Dashboard query (work items with active issues)
SELECT
    wi.id,
    wi.name,
    wi.status,
    wi.phase,
    wi.test_coverage,
    wi.active_blocker_count,
    CASE
        WHEN wi.test_coverage < 0.8 THEN 'low_coverage'
        WHEN wi.active_blocker_count > 0 THEN 'blocked'
        ELSE 'ok'
    END as health_status
FROM work_items wi
WHERE wi.project_id = 1
AND wi.status IN ('in_progress', 'review')
ORDER BY wi.priority ASC, health_status DESC;
```

---

**Document Version**: 1.0
**Last Updated**: 2025-10-17
**Next Review**: After prototype completion
