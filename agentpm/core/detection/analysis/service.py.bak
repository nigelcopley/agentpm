"""
Static Analysis Service - Layer 3 (Detection Services)

Orchestrates static code analysis using Layer 1 AST and metrics utilities.

Architecture Compliance:
- Layer 3: Detection Services (business logic)
- Uses ONLY Layer 1 utilities (ast_utils, metrics_calculator)
- NO direct plugin dependencies
- Stateless service pattern (no database dependencies)

Capabilities:
- Parse Python projects to AST
- Extract code complexity metrics
- Cache parsed results for performance
- Provide metrics aggregation
- Identify high-risk files

Performance Targets (from architecture doc):
- First run: <2s for 100 files
- Cached: <100ms
- Memory: <500MB for large projects

Security:
- NO code execution (uses ast.parse only)
- Safe for untrusted code analysis
- Handles malformed Python gracefully

Usage Example:
    >>> from pathlib import Path
    >>> from agentpm.core.detection.analysis.service import StaticAnalysisService
    >>>
    >>> # Analyze entire project
    >>> service = StaticAnalysisService(Path("/path/to/project"))
    >>> analysis = service.analyze_project()
    >>> print(f"Avg complexity: {analysis.avg_complexity}")
    >>>
    >>> # Find high-risk files
    >>> high_complexity = service.get_high_complexity_files(analysis, threshold=10)
    >>> for file in high_complexity:
    ...     print(f"{file.file_path}: complexity={file.complexity_max}")

Author: APM (Agent Project Manager) Detection Pack
Layer: 3 (Detection Services - Business Logic)
Version: 1.0.0
"""

import hashlib
import json
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime

# Layer 1 imports (utilities)
from agentpm.utils.ast_utils import (
    parse_python_ast,
    extract_classes,
    extract_functions,
)
from agentpm.utils.metrics_calculator import (
    count_lines,
    calculate_cyclomatic_complexity,
    calculate_maintainability_index,
)
from agentpm.utils.ignore_patterns import IgnorePatternMatcher

# Database layer imports (models)
from agentpm.core.database.models.detection_analysis import (
    FileAnalysis,
    ProjectAnalysis,
    ComplexityReport,
    MaintainabilityReport,
)


class AnalysisCache:
    """
    Simple file-based cache for analysis results.

    Invalidates cache entries when file content changes (using SHA-256 hash).

    Attributes:
        cache_dir: Directory for cache files
        enabled: Whether caching is enabled

    Example:
        >>> cache = AnalysisCache(Path(".cache/analysis"))
        >>> cache.set("file.py", file_hash, analysis)
        >>> cached = cache.get("file.py", file_hash)
    """

    def __init__(self, cache_dir: Optional[Path] = None, enabled: bool = True):
        """
        Initialize cache.

        Args:
            cache_dir: Directory for cache files (default: .cache/analysis)
            enabled: Whether caching is enabled
        """
        self.enabled = enabled
        if cache_dir is None:
            cache_dir = Path.cwd() / ".cache" / "analysis"
        self.cache_dir = cache_dir
        if self.enabled:
            self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_cache_path(self, file_path: str) -> Path:
        """Get cache file path for given source file."""
        # Use hash of file path to avoid filesystem issues
        path_hash = hashlib.sha256(file_path.encode()).hexdigest()[:16]
        return self.cache_dir / f"{path_hash}.json"

    def _hash_file(self, file_path: Path) -> str:
        """Calculate SHA-256 hash of file content."""
        return hashlib.sha256(file_path.read_bytes()).hexdigest()

    def get(self, file_path: Path) -> Optional[FileAnalysis]:
        """
        Get cached analysis if valid.

        Args:
            file_path: Path to source file

        Returns:
            Cached FileAnalysis or None if cache miss/invalid
        """
        if not self.enabled:
            return None

        cache_path = self._get_cache_path(str(file_path))
        if not cache_path.exists():
            return None

        try:
            # Load cached data
            cached_data = json.loads(cache_path.read_text())

            # Validate file hash (invalidate if file changed)
            current_hash = self._hash_file(file_path)
            if cached_data.get("file_hash") != current_hash:
                return None

            # Reconstruct FileAnalysis from cached data
            return FileAnalysis(**cached_data["analysis"])

        except (json.JSONDecodeError, KeyError, ValueError):
            # Cache corrupted, ignore
            return None

    def set(self, file_path: Path, analysis: FileAnalysis) -> None:
        """
        Cache analysis result.

        Args:
            file_path: Path to source file
            analysis: Analysis result to cache
        """
        if not self.enabled:
            return

        cache_path = self._get_cache_path(str(file_path))

        try:
            # Store analysis with file hash for validation
            cache_data = {
                "file_hash": self._hash_file(file_path),
                "cached_at": datetime.now().isoformat(),
                "analysis": analysis.model_dump(),
            }
            cache_path.write_text(json.dumps(cache_data, indent=2))

        except (IOError, OSError):
            # Cache write failed, continue without caching
            pass


class StaticAnalysisService:
    """
    Service for static code analysis operations.

    Orchestrates Layer 1 utilities to provide high-level analysis capabilities.

    Architecture Pattern:
    - Layer 3 service (business logic)
    - Uses Layer 1 utilities (ast_utils, metrics_calculator)
    - Stateless (no database dependencies)
    - Supports caching for performance

    Responsibilities:
    - Parse Python projects to AST
    - Extract code metrics
    - Cache results for performance
    - Aggregate metrics across files
    - Identify high-risk files

    Example:
        >>> service = StaticAnalysisService(Path("/my/project"))
        >>> analysis = service.analyze_project()
        >>> print(f"Files: {analysis.total_files}")
        >>> print(f"Complexity: {analysis.avg_complexity:.2f}")
    """

    def __init__(
        self,
        project_path: Path,
        cache_enabled: bool = True,
        cache_dir: Optional[Path] = None,
    ):
        """
        Initialize service.

        Args:
            project_path: Root path of project to analyze
            cache_enabled: Whether to use caching (default: True)
            cache_dir: Custom cache directory (default: .cache/analysis)
        """
        self.project_path = project_path.resolve()
        self.cache = AnalysisCache(cache_dir=cache_dir, enabled=cache_enabled)
        self.ignore_matcher = IgnorePatternMatcher(self.project_path)

    def analyze_file(self, file_path: Path) -> Optional[FileAnalysis]:
        """
        Analyze single Python file.

        Process:
        1. Check cache (if enabled)
        2. Count lines using metrics_calculator
        3. Parse AST using ast_utils
        4. Extract functions and classes
        5. Calculate complexity metrics
        6. Calculate maintainability index
        7. Cache result (if enabled)

        Args:
            file_path: Path to Python file to analyze

        Returns:
            FileAnalysis with all metrics or None if analysis fails

        Example:
            >>> service = StaticAnalysisService(Path("."))
            >>> analysis = service.analyze_file(Path("example.py"))
            >>> if analysis:
            ...     print(f"Complexity: {analysis.complexity_max}")
        """
        # Ensure absolute path
        file_path = file_path.resolve()

        # Check cache first
        cached = self.cache.get(file_path)
        if cached is not None:
            return cached

        try:
            # Step 1: Count lines (Layer 1 utility)
            line_counts = count_lines(file_path)

            # Step 2: Parse AST (Layer 1 utility)
            tree = parse_python_ast(file_path)
            if tree is None:
                # Parse failed, return minimal analysis
                return FileAnalysis(
                    file_path=str(file_path),
                    total_lines=line_counts["total_lines"],
                    code_lines=line_counts["code_lines"],
                    comment_lines=line_counts["comment_lines"],
                    blank_lines=line_counts["blank_lines"],
                    complexity_avg=0.0,
                    complexity_max=0,
                    function_count=0,
                    class_count=0,
                    maintainability_index=0.0,
                )

            # Step 3: Extract functions and classes (Layer 1 utility)
            functions = extract_functions(tree, file_path)
            classes = extract_classes(tree, file_path)

            # Step 4: Calculate complexity (Layer 1 utility)
            complexity_map = calculate_cyclomatic_complexity(tree)

            # Step 5: Aggregate complexity metrics
            if complexity_map:
                complexities = list(complexity_map.values())
                complexity_avg = sum(complexities) / len(complexities)
                complexity_max = max(complexities)
            else:
                complexity_avg = 0.0
                complexity_max = 0

            # Step 6: Calculate maintainability index (Layer 1 utility)
            # MI requires: LOC, cyclomatic complexity, Halstead volume
            # Note: halstead_volume=None triggers simplified approximation (LOC * 0.5)
            mi = calculate_maintainability_index(
                loc=line_counts["code_lines"],
                cyclomatic_complexity=int(complexity_avg) if complexity_avg > 0 else 1,
                halstead_volume=None,  # Use default approximation
            )

            # Step 7: Build FileAnalysis model
            analysis = FileAnalysis(
                file_path=str(file_path),
                total_lines=line_counts["total_lines"],
                code_lines=line_counts["code_lines"],
                comment_lines=line_counts["comment_lines"],
                blank_lines=line_counts["blank_lines"],
                complexity_avg=round(complexity_avg, 2),
                complexity_max=complexity_max,
                function_count=len(functions),
                class_count=len(classes),
                maintainability_index=round(mi, 2),
                functions=functions,
                classes=classes,
            )

            # Cache result
            self.cache.set(file_path, analysis)

            return analysis

        except Exception as e:
            # Analysis failed, return None (graceful degradation)
            print(f"Warning: Failed to analyze {file_path}: {e}")
            return None

    def analyze_project(
        self,
        file_pattern: str = "**/*.py",
        exclude_patterns: Optional[List[str]] = None,
    ) -> ProjectAnalysis:
        """
        Analyze entire project.

        Process:
        1. Find all Python files matching pattern
        2. Filter using IgnorePatternMatcher (respects .gitignore, .agentpmignore, default patterns)
        3. Analyze each file (with caching)
        4. Aggregate metrics
        5. Return ProjectAnalysis

        Args:
            file_pattern: Glob pattern for files (default: **/*.py)
            exclude_patterns: DEPRECATED - Use .agentpmignore file instead.
                            Kept for backward compatibility but ignored.

        Returns:
            Complete ProjectAnalysis with all files

        Example:
            >>> service = StaticAnalysisService(Path("/my/project"))
            >>> analysis = service.analyze_project(file_pattern="**/*.py")
            >>> print(f"Analyzed {analysis.total_files} files")

        Note:
            Filtering now uses IgnorePatternMatcher which respects:
            - .gitignore patterns
            - .agentpmignore patterns (AIPM-specific exclusions)
            - Default patterns (venv/, node_modules/, .git/, etc.)
        """
        # Step 1: Find all Python files
        all_files = list(self.project_path.glob(file_pattern))

        # Step 2: Filter using IgnorePatternMatcher (respects .gitignore, .agentpmignore, defaults)
        files_to_analyze = self.ignore_matcher.filter_paths(all_files)

        # Step 3: Analyze each file
        file_analyses: List[FileAnalysis] = []
        for file_path in files_to_analyze:
            analysis = self.analyze_file(file_path)
            if analysis is not None:
                file_analyses.append(analysis)

        # Step 4: Aggregate metrics
        if file_analyses:
            total_lines = sum(f.total_lines for f in file_analyses)
            total_code_lines = sum(f.code_lines for f in file_analyses)

            # Average complexity (weighted by code lines)
            if total_code_lines > 0:
                weighted_complexity = sum(
                    f.complexity_avg * f.code_lines for f in file_analyses
                )
                avg_complexity = weighted_complexity / total_code_lines
            else:
                avg_complexity = 0.0

            # Maximum complexity across all files
            max_complexity = max((f.complexity_max for f in file_analyses), default=0)

            # Average maintainability (weighted by code lines)
            if total_code_lines > 0:
                weighted_mi = sum(
                    f.maintainability_index * f.code_lines for f in file_analyses
                )
                avg_maintainability = weighted_mi / total_code_lines
            else:
                avg_maintainability = 0.0

        else:
            total_lines = 0
            total_code_lines = 0
            avg_complexity = 0.0
            max_complexity = 0
            avg_maintainability = 0.0

        # Step 4: Build ProjectAnalysis
        return ProjectAnalysis(
            project_path=str(self.project_path),
            total_files=len(file_analyses),
            total_lines=total_lines,
            total_code_lines=total_code_lines,
            avg_complexity=round(avg_complexity, 2),
            max_complexity=max_complexity,
            avg_maintainability=round(avg_maintainability, 2),
            files=file_analyses,
            analyzed_at=datetime.now(),
        )

    def get_high_complexity_files(
        self,
        analysis: ProjectAnalysis,
        threshold: int = 10,
    ) -> List[FileAnalysis]:
        """
        Identify files exceeding complexity threshold.

        Files are sorted by max complexity (highest first).

        Args:
            analysis: ProjectAnalysis to filter
            threshold: Complexity threshold (default: 10)

        Returns:
            List of FileAnalysis exceeding threshold, sorted by complexity

        Example:
            >>> service = StaticAnalysisService(Path("."))
            >>> analysis = service.analyze_project()
            >>> high_complexity = service.get_high_complexity_files(analysis, threshold=10)
            >>> for file in high_complexity:
            ...     print(f"{file.file_path}: {file.complexity_max}")
        """
        high_complexity = [
            f for f in analysis.files
            if f.complexity_max > threshold
        ]

        # Sort by complexity (highest first)
        return sorted(
            high_complexity,
            key=lambda f: f.complexity_max,
            reverse=True,
        )

    def get_low_maintainability_files(
        self,
        analysis: ProjectAnalysis,
        threshold: float = 65.0,
    ) -> List[FileAnalysis]:
        """
        Identify files below maintainability threshold.

        MI Scale:
        - 85-100: Excellent
        - 65-84: Good
        - <65: Needs attention

        Files are sorted by MI (lowest first).

        Args:
            analysis: ProjectAnalysis to filter
            threshold: MI threshold (default: 65.0)

        Returns:
            List of FileAnalysis below threshold, sorted by MI

        Example:
            >>> service = StaticAnalysisService(Path("."))
            >>> analysis = service.analyze_project()
            >>> low_mi = service.get_low_maintainability_files(analysis, threshold=65.0)
            >>> for file in low_mi:
            ...     print(f"{file.file_path}: MI={file.maintainability_index:.1f}")
        """
        low_maintainability = [
            f for f in analysis.files
            if f.maintainability_index < threshold
        ]

        # Sort by MI (lowest first)
        return sorted(
            low_maintainability,
            key=lambda f: f.maintainability_index,
        )

    def generate_complexity_report(
        self,
        analysis: ProjectAnalysis,
        threshold: int = 10,
        top_n: int = 10,
    ) -> ComplexityReport:
        """
        Generate detailed complexity report.

        Identifies high-complexity files and top complexity hotspots.

        Args:
            analysis: ProjectAnalysis to report on
            threshold: Complexity threshold (default: 10)
            top_n: Number of hotspots to include (default: 10)

        Returns:
            ComplexityReport with violations and hotspots

        Example:
            >>> service = StaticAnalysisService(Path("."))
            >>> analysis = service.analyze_project()
            >>> report = service.generate_complexity_report(analysis)
            >>> print(f"Violations: {report.total_violations}")
        """
        high_complexity_files = self.get_high_complexity_files(analysis, threshold)

        # Extract all functions from all files
        all_functions: List[Dict[str, Any]] = []
        for file_analysis in analysis.files:
            for func in file_analysis.functions:
                # Add file path for context
                func_with_file = func.copy()
                func_with_file["file_path"] = file_analysis.file_path
                all_functions.append(func_with_file)

        # Sort by complexity and take top N
        hotspots = sorted(
            all_functions,
            key=lambda f: f.get("complexity", 0),
            reverse=True,
        )[:top_n]

        return ComplexityReport(
            threshold=threshold,
            high_complexity_files=high_complexity_files,
            hotspots=hotspots,
            total_violations=len(high_complexity_files),
        )

    def generate_maintainability_report(
        self,
        analysis: ProjectAnalysis,
        threshold: float = 65.0,
    ) -> MaintainabilityReport:
        """
        Generate detailed maintainability report.

        Identifies files below maintainability threshold.

        Args:
            analysis: ProjectAnalysis to report on
            threshold: MI threshold (default: 65.0)

        Returns:
            MaintainabilityReport with violations

        Example:
            >>> service = StaticAnalysisService(Path("."))
            >>> analysis = service.analyze_project()
            >>> report = service.generate_maintainability_report(analysis)
            >>> print(f"Violations: {report.total_violations}")
        """
        low_maintainability_files = self.get_low_maintainability_files(
            analysis,
            threshold,
        )

        return MaintainabilityReport(
            threshold=threshold,
            low_maintainability_files=low_maintainability_files,
            total_violations=len(low_maintainability_files),
        )
